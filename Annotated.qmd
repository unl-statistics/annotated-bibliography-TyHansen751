---
title: "Annotated Bibliography: Deep Learning from a Statistical Perspective"
author: Ty Hansen
date: 2026-01-28
bibliography: AnnotatedBib.bib
csl: chicago-syllabus.csl
suppress-bibliography: true
link-citations: false
citations-hover: false
---

- @goodfellowExplainingHarnessingAdversarial2015
    - Summary: Neural networks are really good at recognizing things like images, but tiny, sneaky changes to an image (that humans barely notice) can trick them into confidently giving the wrong answer. The paper says this happens mostly because networks act "linear" in high dimensions. They show a super simple trick (Fast Gradient Sign Method) to create these tricky examples and explain that training the network on them makes it tougher and sometimes even more accurate overall.
    
    - Reliability assessment: Written by top experts, Goofellow credited with created GANS, a technique that uses neural networks to create realistic images and content.It's a hugely influential paper that is cited over 28 thousand times, in articles related to deep learning.
    
    - Missing information:
       To fully understand why small changes cause big mistakes, you need some basics of high dimensional spaces and how tiny tweaks add up. The paper also assumes you know what a decision boundary is in a neural network.

- @lecunLearningProcessAsymmetric1986
    - Summary: Back in the 1980s, simple networks could only learn easy, straight line patterns. This paper shows an early way to train deeper networks with "hidden" units in between. It uses backpropagation, you calculate the error, send it backward through the units, and tweak the connections so the network learns complicated, curvy patterns.
    
    - Yann LeCun is one of the godfathers of modern deep learning (cheif scientist of Facebook MetaAI). This is one of the very first papers describing backpropagation. The atricle cites it as a key historical step showing how gradient based training became the main way to teach neural networks.
    
    - Missing information:
       To fully understand the learning process, you need a solid foundation of iterative minimization equations and theories. Equations dealing with vector and minimize cost functions, as well as matrices need full understanding. 
